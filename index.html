<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Beyond Self-Driving: Exploring Three Levels of Driving Automation | ICCV 2025</title>
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/assets/css/fontawesome.min.css" />
  <link rel="stylesheet" href="/assets/css/style.css" />
  <link rel="icon" href="/assets/favicon.ico" type="image/x-icon" />
  <script src="/assets/js/jquery.min.js"></script>
  <script src="/assets/js/bootstrap.min.js"></script>
</head>
<body>
  <div id="header" class="navbar navbar-expand-lg fixed-top">
    <div class="container">
      <div class="navbar-brand">
          <img src="/assets/images/logo.png" alt="Logo">
      </div> <!-- #logo -->
      <button type="button" class="navbar-toggler" data-bs-toggle="offcanvas" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div id="navbar" class="offcanvas offcanvas-end">
        <div class="offcanvas-header border-bottom">
          <h5 class="offcanvas-title">Menu</h5>
          <button type="button" class="btn-close" data-bs-dismiss="offcanvas" aria-label="Close"></button>
        </div> <!-- .offcanvas-header -->
        <div class="offcanvas-body">
          <ul class="navbar-nav">
            <li class="nav-item"><a href="#intro">Introduction</a></li>
            <li class="nav-item"><a href="#schedule">Schedule</a></li>
            <li class="nav-item"><a href="#organizers">Organizers</a></li>
          </ul>
        </div> <!-- .offcanvas-body -->
      </div> <!-- .offcanvas -->
    </div> <!-- .container -->
  </div> <!-- #header -->
  <div id="content">
    <div id="hero">
      <div class="container">
        <h1>Beyond Self-Driving: Exploring Three Levels of Driving Automation</h1>
        <h5>ICCV 2025 Tutorial</h5>
        <p>
          <i class="fa fa-calendar-days"></i> TBD <br>
          <i class="fa fa-location-dot"></i> TBD
        </p>
      </div> <!-- .container -->
    </div> <!-- #hero -->
    <div id="intro" class="section">
      <div class="container">
        <h2>Introduction</h2>
        <p>
          Self-driving technologies have demonstrated significant potential to transform human mobility. However, single-agent systems face inherent limitations in perception and decision-making. Transitioning from self-driving vehicles to cooperative multi-vehicle systems and large-scale intelligent transportation systems is essential to enable safer and more efficient mobility. However, realizing such sophisticated mobility systems introduces significant challenges, requiring comprehensive tools and models, simulation environments, and real-world datasets and deployment frameworks.
          This tutorial will delve into key areas of driving automation, beginning with advanced end-to-end techniques such as vision-language-action (VLA) models, behavior prediction, and scenario generation. The tutorial emphasizes V2X communication and real-world cooperative perception through systems like OpenCDA and V2X-ReaLO, and datasets including V2X-Real and V2XPnP. It also covers simulation deployment frameworks and urban-scale mobility simulations, such as MetaDrive, MetaUrban, and UrbanSim. This comprehensive session aims to bridge foundational research with real-world deployment, providing practical insights for building future-ready autonomous mobility systems.
        </p>
      </div> <!-- .container -->
    </div> <!-- .section -->
    <div id="schedule" class="section">
      <div class="container">
        <h2>Schedule</h2>
        <table class="table table-striped">
          <thead>
            <tr>
              <th scope="col">Time (GMT-5)</th>
              <th scope="col">Programme</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>09:20 - 09:30</td>
              <td>Opening Remarks</td>
            </tr>
            <tr>
              <td>09:40 - 10:20</td>
              <td class="programme">
                <p class="title">Invited Talk: Scaling Foundation World Models as a Path to Embodied AGI</p>
                <ul class="list-inline">
                  <li class="list-inline-item"><a href="javascript:void(0)">[Abstract]</a></li>
                </ul>
                <p class="abstract">Simulation offers tremendous promise for training and evaluating agents in diverse controllable settings, yet, we remain far from building a simulator that is anywhere near as rich as the real world. But what if we could learn one? With the advent of foundation world models we are now entering a new era, where we can train world models from vast quantities of passively acquired data such as internet videos. These foundation world models can then generate unlimited environments for agents, offering the path to open-ended curricula. This talk outlines progress in this space over the past two years, while looking ahead to the future.</p>
                <p class="bio">Jack Parker-Holder is a Research Scientist at Google DeepMind where he works on foundation world models. Jack co-led the Genie project which won the Best Paper Award at ICML 2024 and was the lead of Genie 2. He is also an honorary lecturer at University College London where he co-leads the Open-Endedness and General Intelligence module. Prior to Google DeepMind Jack completed his DPhil at the University of Oxford where his research focused on world models and open-ended learning.</p>
              </td>
              </td>
            </tr>
            <tr>
              <td>10:20 - 10:40</td>
              <td>Coffee Break</td>
            </tr>
            <tr>
              <td>10:40 - 11:20</td>
              <td class="programme">
                <p class="title">Invited Talk: Physics-Grounded World Models: Generation, Interaction, and Evaluation</p>
                <ul class="list-inline">
                  <li class="list-inline-item"><a href="javascript:void(0)">[Abstract]</a></li>
                  <li class="list-inline-item"><a target="_blank" href="/assets/slides/koven.pdf">[Slides]</a></li>
                </ul>
                <p class="abstract">Video generation models, by learning to synthesize pixels from large-scale real-world data, have shown great promise as world models. Yet, pixel-only approaches face critical challenges: they struggle with precise action control, cannot guarantee physical consistency, and suffer from computational inefficiency—fundamentally limiting how users and agents can interact with these virtual worlds. At the heart of these limitations lies a crucial missing piece: explicit physical grounding. In this talk, I will present how we address this gap through physics-grounded world models: WonderWorld enables fast, interactive world generation and real-time exploration by introducing a physics-based representation, and WonderPlay extends this to dynamic action control by integrating physics simulation with video generation. Finally, I will introduce WorldScore, a unified benchmark that evaluates world models—spanning 3D, 4D, and video generation—on their ability to generate controllable, consistent, and dynamic worlds. These works outline a path towards interactive world models by synergizing neural pixel generation and physical understanding.</p>
                <p class="bio">Hong-Xing "Koven" Yu is a PhD candidate at the Computer Science Department of Stanford University, advised by Prof. Jiajun Wu. His research interest centers around how AI can understand and generate the physical world. He is a recipient of the SIGGRAPH Asia Best Paper Award, the Stanford SoE Fellowship, the Qualcomm Innovation Fellowship, and the Meshy Fellowship, and a finalist of the NVIDIA Fellowship, the Meta Fellowship, the Jane Street Fellowship, and the Roblox Fellowship.</p>
              </td>
            </tr>
            <tr>
              <td>11:20 - 13:30</td>
              <td>Lunch Break</td>
            </tr>
            <tr>
              <td>13:30 - 14:10</td>
              <td class="programme">
                <p class="title">Invited Talk: Breaking the Algorithmic Ceiling in Pre-Training with an Inference-first Perspective</p>
                <ul class="list-inline">
                  <li class="list-inline-item"><a href="javascript:void(0)">[Abstract]</a></li>
                  <li class="list-inline-item"><a target="_blank" href="/assets/slides/jiaming.pdf">[Slides]</a></li>
                </ul>
                <p class="abstract">Recent years have seen significant advancements in foundation models through generative pre-training, yet algorithmic innovation in this space has largely stagnated around autoregressive models for discrete signals and diffusion models for continuous signals. This stagnation creates a bottleneck that prevents us from fully unlocking the potential of rich multi-modal data, which in turn limits the progress on multimodal intelligence. We argue that an inference-first perspective, which prioritizes scaling efficiency during inference time across sequence length and refinement steps, can inspire novel generative pre-training algorithms. Using Inductive Moment Matching (IMM) as a concrete example, we demonstrate how addressing limitations in diffusion models' inference process through targeted modifications yields a stable, single-stage algorithm that achieves superior sample quality with over an order of magnitude greater inference efficiency.</p>
                <p class="bio">Jiaming Song is the Chief Scientist in Luma AI, where he is working on next-generation multimodal foundation models. He received his Ph.D. at Stanford University, under the supervision of Stefano Ermon. He has developed a few early works on diffusion models, such as DDIM. He is the recipient of the ICLR 2022 Outstanding Paper Award.</p>
              </td>
            </tr>
            <tr>
              <td>14:10 - 14:20</td>
              <td>Coffee Break</td>
            </tr>
            <tr>
              <td>14:20 - 15:00</td>
              <td class="programme">
                <p class="title">Invited Talk: An Introduction to Kling and Our Research towards More Powerful Video Generation Models</p>
                <ul class="list-inline">
                  <li class="list-inline-item"><a href="javascript:void(0)">[Abstract]</a></li>
                </ul>
                <p class="abstract">We introduce modern video generation and world model technologies using Kling, Kuaishou's video generation model, as an example. We will firstly provide a brief overview of Kling's main capabilities and features, and then delve into our ongoing research in four key directions: 1) advancing model architecture and generative AI algorithms; 2) enhancing powerful interactive and control capacities; 3) incorporating accurate evaluation and alignment mechanisms; and 4) improving multimodal perception and reasoning. We hope this tutorial will help the audience better understand Kling and our vision for future video generation and world models.</p>
                <p class="bio">Pengfei Wan is the head of Visual Generation and Interaction Center (aka the Kling team), Kuaishou Technology. He obtained the PhD degree from HKUST in 2015. He has long been committed to R&D of intelligent content creation and immersive interaction systems. His team developed the Kling video generation models that have over 20 million users worldwide.</p>
              </td>
            </tr>
            <tr>
              <td>15:00 - 15:10</td>
              <td>Coffee Break</td>
            </tr>
            <tr>
              <td>15:10 - 15:50</td>
              <td class="programme">
                <p class="title">Invited Talk: Streaming Perception: Towards Learning Structured Models of the World</p>
                <ul class="list-inline">
                  <li class="list-inline-item"><a href="javascript:void(0)">[Abstract]</a></li>
                </ul>
                <p class="abstract">In this talk, I’ll reflect on the rise of video models and what it means to learn a world model. Recent video models are reaching an uncanny level of visual realism. They certainly look like they understand the world. They’re getting remarkably good at rendering (!). But how do we make that understanding more concrete, more structured—something we can reason with and act on? I’ll suggest that getting there might require streaming perception: systems that build structured, persistent internal representations from continuous input. I’ll share recent works like CUT3R and ST4rtrack as steps in this direction, and offer a perspective shaped by my recent experience raising a small human.</p>
                <p class="bio">Angjoo Kanazawa is an Assistant Professor in the Department of Electrical Engineering and Computer Sciences at the University of California, Berkeley. She develops methods for perceiving, understanding, and interacting with the dynamic 3D world behind everyday images and video. Her research has been recognized with honors including the Google Research Scholar Award, Sloan Fellowship, the PAMI Young Researcher Award and the NSF CAREER award. Prior to joining Berkeley, she completed her Ph.D. at the University of Maryland, College Park, and spent time at the Max Planck Institute for Intelligent Systems and Google Research.</p>
              </td>
            </tr>
            <tr>
              <td>15:50 - 16:00</td>
              <td>Coffee Break</td>
            </tr>
            <tr>
              <td>16:00 - 16:40</td>
              <td class="programme">
                <p class="title">Invited Talk: Scaling World Models for Agents</p>
                <ul class="list-inline">
                  <li class="list-inline-item"><a href="javascript:void(0)">[Abstract]</a></li>
                  <li class="list-inline-item"><a target="_blank" href="/assets/slides/sherry.pdf">[Slides]</a></li>
                </ul>
                <p class="abstract">World modeling through video generation has great potential including being used to train general-purpose agents. In this tutorial, we will first discuss how to build world models that can emulate a diverse set of real-world environments in reaction to different types of action inputs. We then discuss how a world model can be used for long-horizon planning, as well as for evaluating and improving embodied agents. Lastly, we will discuss how to improve the world model itself through Reinforcement Learning from external feedback and Iterative learning and data generation.</p>
                <p class="bio">Sherry Yang is an incoming Assistant Professor of Computer Science at NYU Courant and a Staff Research Scientist at Google DeepMind. She researches in machine learning with a focus on reinforcement learning and generative modeling. Her current research interests include learning world models and agents, and their applications in robotics and AI for science. Her research has been recognized by the Best Paper award at ICLR and media outlets such as VentureBeat and TWIML. She has organized tutorials, workshops, and served as Area Chairs at major conferences (NeurIPS, ICLR, ICML, CVPR). Prior to her current role, she was a post-doc at Stanford working with Percy Liang. She received her Ph.D. from UC Berkeley advised by Pieter Abbel and Master’s and Bachelor's degrees from MIT.</p>
              </td>
            </tr>
            <tr>
              <td>16:40 - 16:50</td>
              <td>Ending Remarks (Lucky Draw)</td>
            </tr>
          </tbody>
        </table>
      </div> <!-- .container -->
    </div> <!-- .section -->
    <div id="organizers" class="section">
      <div class="container">
        <h2>Organizers</h2>
        <div class="row">
          <div class="col-lg-3 col-md-4 col-6">
            <div class="card">
              <img src="/assets/images/avatars/zhiyu.webp" class="card-img-top">
              <div class="card-body">
                <h5 class="card-title">Zhiyu Huang</h5>
                <p class="card-text">UCLA</p>
                <ul class="list-inline">
                  <li class="list-inline-item"><a href="https://mczhi.github.io/" target="_blank"><i class="fa fa-house"></i></a></li>
                  <li class="list-inline-item"><a href="https://scholar.google.com/citations?user=aLZEVCsAAAAJ&hl=en" target="_blank"><i class="fa-brands fa-google-scholar"></i></a></li>
                  <li class="list-inline-item"><a href="https://github.com/MCZhi" target="_blank"><i class="fa-brands fa-github"></i></a></li>
                </ul>
              </div> <!-- .card-body -->
            </div> <!-- .card -->
          </div> <!-- .col -->
          <div class="col-lg-3 col-md-4 col-6">
            <div class="card">
              <img src="/assets/images/avatars/weichen.webp" class="card-img-top">
              <div class="card-body">
                <h5 class="card-title">Wayne Wu</h5>
                <p class="card-text">UCLA</p>
                <ul class="list-inline">
                  <li class="list-inline-item"><a href="https://wywu.github.io/" target="_blank"><i class="fa fa-house"></i></a></li>
                  <li class="list-inline-item"><a href="https://scholar.google.com/citations?user=uWfZKz4AAAAJ&hl=en" target="_blank"><i class="fa-brands fa-google-scholar"></i></a></li>
                  <li class="list-inline-item"><a href="//github.com/WeichenFan" target="_blank"><i class="fa-brands fa-github"></i></a></li>
                </ul>
              </div> <!-- .card-body -->
            </div> <!-- .card -->
          </div> <!-- .col -->
          <div class="col-lg-3 col-md-4 col-6">
            <div class="card">
              <img src="/assets/images/avatars/haozhe.webp" class="card-img-top">
              <div class="card-body">
                <h5 class="card-title">Haozhe Xie</h5>
                <p class="card-text">MMLab@NTU</p>
                <ul class="list-inline">
                  <li class="list-inline-item"><a href="//haozhexie.com/about" target="_blank"><i class="fa fa-house"></i></a></li>
                  <li class="list-inline-item"><a href="//scholar.google.com/citations?user=b3EiE-IAAAAJ" target="_blank"><i class="fa-brands fa-google-scholar"></i></a></li>
                  <li class="list-inline-item"><a href="//github.com/hzxie" target="_blank"><i class="fa-brands fa-github"></i></a></li>
                </ul>
              </div> <!-- .card-body -->
            </div> <!-- .card -->
          </div> <!-- .col -->
          <div class="col-lg-3 col-md-4 col-6">
            <div class="card">
              <img src="/assets/images/avatars/fangzhou.webp" class="card-img-top">
              <div class="card-body">
                <h5 class="card-title">Fangzhou Hong</h5>
                <p class="card-text">MMLab@NTU</p>
                <ul class="list-inline">
                  <li class="list-inline-item"><a href="//hongfz16.github.io" target="_blank"><i class="fa fa-house"></i></a></li>
                  <li class="list-inline-item"><a href="//scholar.google.com/citations?user=mhaiL5MAAAAJ" target="_blank"><i class="fa-brands fa-google-scholar"></i></a></li>
                  <li class="list-inline-item"><a href="//github.com/hongfz16" target="_blank"><i class="fa-brands fa-github"></i></a></li>
                </ul>
              </div> <!-- .card-body -->
            </div> <!-- .card -->
          </div> <!-- .col -->
          <div class="col-lg-3 col-md-4 col-6">
            <div class="card">
              <img src="/assets/images/avatars/ziqi.webp" class="card-img-top">
              <div class="card-body">
                <h5 class="card-title">Ziqi Huang</h5>
                <p class="card-text">MMLab@NTU</p>
                <ul class="list-inline">
                  <li class="list-inline-item"><a href="//ziqihuangg.github.io" target="_blank"><i class="fa fa-house"></i></a></li>
                  <li class="list-inline-item"><a href="//scholar.google.com/citations?user=Y3h_pzMAAAAJ" target="_blank"><i class="fa-brands fa-google-scholar"></i></a></li>
                  <li class="list-inline-item"><a href="//github.com/ziqihuangg" target="_blank"><i class="fa-brands fa-github"></i></a></li>
                </ul>
              </div> <!-- .card-body -->
            </div> <!-- .card -->
          </div> <!-- .col -->
          <div class="col-lg-3 col-md-4 col-6">
            <div class="card">
              <img src="/assets/images/avatars/jiajun.webp" class="card-img-top">
              <div class="card-body">
                <h5 class="card-title">Jiajun Wu</h5>
                <p class="card-text">Stanford</p>
                <ul class="list-inline">
                  <li class="list-inline-item"><a href="//jiajunwu.com" target="_blank"><i class="fa fa-house"></i></a></li>
                  <li class="list-inline-item"><a href="//scholar.google.com/citations?user=2efgcS0AAAAJ" target="_blank"><i class="fa-brands fa-google-scholar"></i></a></li>
                  <li class="list-inline-item"><a href="//github.com/jiajunwu" target="_blank"><i class="fa-brands fa-github"></i></a></li>
                </ul>
              </div> <!-- .card-body -->
            </div> <!-- .card -->
          </div> <!-- .col -->
          <div class="col-lg-3 col-md-4 col-6">
            <div class="card">
              <img src="/assets/images/avatars/ziwei.webp" class="card-img-top">
              <div class="card-body">
                <h5 class="card-title">Ziwei Liu</h5>
                <p class="card-text">MMLab@NTU</p>
                <ul class="list-inline">
                  <li class="list-inline-item"><a href="//liuziwei7.github.io" target="_blank"><i class="fa fa-house"></i></a></li>
                  <li class="list-inline-item"><a href="//scholar.google.com/citations?user=lc45xlcAAAAJ" target="_blank"><i class="fa-brands fa-google-scholar"></i></a></li>
                  <li class="list-inline-item"><a href="//github.com/liuziwei7" target="_blank"><i class="fa-brands fa-github"></i></a></li>
                </ul>
              </div> <!-- .card-body -->
            </div> <!-- .card -->
          </div> <!-- .col -->
          <div class="col-lg-3 col-md-4 col-6">
            <div class="card">
              <img src="/assets/images/kwai.svg" class="card-img-top card-img-square">
              <div class="card-body">
                <h5 class="card-title">Kuaishou</h5>
                <p class="card-text">Advisory Organizer</p>
                <ul class="list-inline">
                  <li class="list-inline-item"><a href="//www.kuaishou.com/en" target="_blank"><i class="fa fa-house"></i></a></li>
                </ul>
              </div> <!-- .card-body -->
            </div> <!-- .card -->
          </div> <!-- .col -->
        </div> <!-- .row -->
      </div> <!-- .container -->
    </div> <!-- .section -->
  </div> <!-- #content -->
  <div id="footer">
    <div class="container">
      <img src="/assets/images/logo.png">
      <p>&copy;2025 <i>Beyond Self-Driving: Exploring Three Levels of Driving Automation</i> Tutorial</p>
    </div> <!-- .container -->
  </div> <!-- #footer -->
  <script type="text/javascript">
    $(document).on('scroll', function() {
      let currentOffset = $(window).scrollTop() + 80,
          currentSection = "",
          sections = ["intro", "schedule", "organizers"];

      for (let i = 0; i < sections.length; ++i) {
        if (currentOffset < $('#' + sections[i]).offset().top) {
          break;
        }
        currentSection = sections[i];
      }

      let $navbar = $('.navbar'),
          $navLinks = $navbar.find('a');

      $navLinks.parent().removeClass('active');
      $navLinks.each(function() {
        if ($(this).attr('href') === '#' + currentSection) {
          $(this).parent().addClass('active');
        }
      });
    });
  </script>
  <script type="text/javascript">
    $(".programme").on("click", ".list-inline-item", function(e) {
      if ($(this).text() === "[Abstract]") {
        $(this).parent().siblings(".bio").hide();
        $(this).parent().siblings(".abstract").toggle();
      }
    })
  </script>
</body>
</html>
